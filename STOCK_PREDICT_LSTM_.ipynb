{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1 Year Data\n",
        "\n",
        "1. 5args\n",
        "2. 7args\n",
        "3. 加大盤\n",
        "4. 加買超賣超\n",
        "\n",
        "\n",
        "*   5/20\n",
        "*   10/20\n",
        "*   20/10\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9soNYM815BHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "ITpKz-2Gm3Kq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPsY_d0HP8Xv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "804e065e-e40b-4487-8246-e66fa5c4f30b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import math\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "%matplotlib inline\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FIVE_ARGS = ['Open', 'Close', 'High', 'Low', 'Volume']\n",
        "SIX_ARGS = ['Open', 'Close', 'High', 'Low', 'Volume', 'II']\n",
        "SEVEN_ARGS = ['Open', 'Close', 'High', 'Low', 'Volume', 'SMA', 'BIAS']\n",
        "EIGHT_ARGS = ['Open', 'Close', 'High', 'Low', 'Volume', 'SMA', 'BIAS', 'II']\n",
        "\n",
        "home = '/content/drive/MyDrive/find_optimal/'\n",
        "params_dir = 'final_model_params'\n",
        "one_year_path = home + '0050-KY/'\n",
        "all_year_path = home + '0050-K/'\n",
        "\n",
        "SMA_num = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read final model csv and initialize directory\n"
      ],
      "metadata": {
        "id": "lMVhxXN00Ftr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "company_list = ['智邦', '中信金', '統一超', '玉山金', '鴻海', '日月光投控', '亞德客-KY', '瑞昱', '中鋼', '聯發科', '大立光', '南亞科', '台泥', '緯創', '兆豐金', '合庫金', '和碩', '第一金', '遠傳', '豐泰', '富邦金', '台積電', '永豐金', '元大金', '台塑', '欣興', '開發金', '和泰車', '統一', '長榮', '緯穎', '廣達', '上海商銀', '台塑化', '南亞', '國巨', '中租-KY', '華南金', '彰銀', '台新金', '國泰金', '研華', '華碩', '聯詠', '台化', '台灣大', '中華電', '光寶科', '聯電', '台達電']\n",
        "company_list = company_list[10:20]\n",
        "\n",
        "final_model = pd.read_csv(home + 'final_model.csv')\n",
        "final_model.columns = ['company', 'input', 'args', 'mse', 'model', 'days' , 'comment1', 'start', 'comment2']\n",
        "final_model = final_model.drop(['mse', 'comment1', 'comment2'], axis=1).loc[1:, :].reset_index(drop=True)\n",
        "print(final_model)\n",
        "\n",
        "if params_dir not in os.listdir(home):\n",
        "  os.mkdir(home+params_dir)\n",
        "\n",
        "for company in company_list:\n",
        "  if company not in os.listdir(home+params_dir):\n",
        "    os.mkdir(home+params_dir+'/'+company)\n",
        "  for f in os.listdir(home+params_dir+'/'+company):\n",
        "    os.remove(home+params_dir+'/'+company+'/'+f)\n",
        "\n",
        "params_path = home + params_dir + '/'\n",
        "\n"
      ],
      "metadata": {
        "id": "3eIMUuFw0Im0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6db9587e-a6de-47b9-a19a-726191f3b15b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   company input args model    days start\n",
            "0       智邦    20    5     7     all   NaN\n",
            "1      中信金    10    6     4     all   NaN\n",
            "2      大立光     5    5     4     one   NaN\n",
            "3    日月光投控     5    7     1     one   NaN\n",
            "4      統一超     5    8     6     one   NaN\n",
            "5      南亞科    10    8     4     one   NaN\n",
            "6   亞德客-KY     5    7     7  select  2500\n",
            "7       鴻海     5    6     6     one  3500\n",
            "8       中鋼    20    7     4     one   NaN\n",
            "9      聯發科    10    5     5     all   NaN\n",
            "10      瑞昱     5    7     4  select  4500\n",
            "11      緯創    20    5     5     all   NaN\n",
            "12     兆豐金    20    7     7     one  4000\n",
            "13     國泰金     5    7     1     one   NaN\n",
            "14      台泥    10    5     5     all   NaN\n",
            "15      聯詠    20    7     1     one   NaN\n",
            "16      台化     5    5     1     one   NaN\n",
            "17      華碩     5    5     4     all   NaN\n",
            "18      研華    20    6     4     one   NaN\n",
            "19     玉山金     5    6     1     one   NaN\n",
            "20      緯穎    20    8     1     one   NaN\n",
            "21      廣達    20    5     5     all  3000\n",
            "22      南亞    10    6     1     one   NaN\n",
            "23     台塑化     5    5     4     one   NaN\n",
            "24    上海商銀    10    5     1     one   NaN\n",
            "25     台新金     5    5     4     one   NaN\n",
            "26     台達電    20    7     5     all   NaN\n",
            "27      彰銀    10    5     1     one   NaN\n",
            "28     華南金     5    7     1     one   NaN\n",
            "29   中租-KY     5    7     7     one   NaN\n",
            "30      國巨     5    8     1     one   NaN\n",
            "31      聯電     5    7     4  select  5300\n",
            "32     光寶科     5    5     5     all   NaN\n",
            "33     中華電     5    5     1     one   NaN\n",
            "34     台灣大    10    7     1     one   NaN\n",
            "35      長榮    20    7     1     one   NaN\n",
            "36      台塑    20    5     1     one   NaN\n",
            "37      豐泰     5    7     4  select  3800\n",
            "38      統一     5    5     1     one   NaN\n",
            "39     元大金     5    8     1     one   NaN\n",
            "40     永豐金     5    5     4  select  3000\n",
            "41      遠傳     5    6     1     one   NaN\n",
            "42     台積電    10    8     1     one   NaN\n",
            "43     第一金     5    6     1     one   NaN\n",
            "44      和碩     5    6     1     one   NaN\n",
            "45     合庫金     5    8     1     one   NaN\n",
            "46      欣興     5    5     1     one   NaN\n",
            "47     富邦金    10    7     4     one   NaN\n",
            "48     和泰車     5    5     4     one   NaN\n",
            "49     開發金    20    7     1     one   NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input CSV File"
      ],
      "metadata": {
        "id": "T518sboKm5mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "company_df_dict = {}\n",
        "for company in company_list:\n",
        "  # print(company)\n",
        "  if final_model.loc[final_model['company'] == company, 'days'].values[0] == 'one':\n",
        "    df = pd.read_csv(one_year_path+company+'/K.csv')\n",
        "    df = df.iloc[2:, 1:16]\n",
        "    df = df.drop(['漲跌','漲跌.1','振幅','Unnamed: 9','Unnamed: 10','Unnamed: 11','法人買賣超(張)','Unnamed: 13','Unnamed: 14'],axis=1)\n",
        "\n",
        "    df.columns = ['Open','High','Low','Close','Volume','II']\n",
        "    df['Volume'] = df['Volume'].apply(lambda x: float(str(x).replace(',','')))\n",
        "    df['II'] = df['II'].apply(lambda x: float(str(x).replace(',','')))\n",
        "    df = df.iloc[::-1].reset_index(drop=True)\n",
        "    if int(final_model.loc[final_model['company'] == company, 'args'].values[0]) == 5 or int(final_model.loc[final_model['company'] == company, 'args'].values[0]) == 7:\n",
        "      df = df.drop(['II'], axis=1)\n",
        "\n",
        "  else:\n",
        "    df = pd.read_csv(all_year_path+company+'/K.csv')\n",
        "    df = df.iloc[:, 1:]\n",
        "    df = df.drop(['Adj Close'],axis=1)\n",
        "    df = df.dropna()\n",
        "    df = df.loc[df['Volume'] != 0.0].reset_index(drop=True)\n",
        "\n",
        "  if int(final_model.loc[final_model['company'] == company, 'args'].values[0]) >= 7:\n",
        "    SMA = []\n",
        "    BIAS = []\n",
        "    for i in range(len(df)):\n",
        "      if i < SMA_num - 1:\n",
        "        SMA.append(0)\n",
        "        BIAS.append(0)\n",
        "      else:\n",
        "        SMA.append(df.loc[i+1-SMA_num:i+1, 'Open'].mean())\n",
        "        BIAS.append((df.loc[i, 'Open'] - SMA[i]) / SMA[i])\n",
        "    df['SMA'] = SMA\n",
        "    df['BIAS'] = BIAS\n",
        "    df = df.loc[SMA_num:].reset_index(drop=True)\n",
        "\n",
        "  if final_model.loc[final_model['company'] == company, 'days'].values[0] == 'select':\n",
        "    df = df[int(final_model.loc[final_model['company'] == company, 'start'].values[0]):].reset_index(drop=True)\n",
        "\n",
        "  # print(df.head())\n",
        "  company_df_dict[company] = df"
      ],
      "metadata": {
        "id": "ouHqVkCgRIXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions"
      ],
      "metadata": {
        "id": "wBRzJ3l9wMdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Split_Data(df, input_num, predict_num, split_point):\n",
        "  train_days = split_point-input_num+1\n",
        "  X_train = []\n",
        "  Y_train = []\n",
        "\n",
        "  for i in range(train_days):\n",
        "    X_train.append(df.loc[i:i+input_num-1, :].values)\n",
        "    Y_train.append([df.loc[i+input_num+predict_num-1, 'Open']])\n",
        "\n",
        "  return np.array(X_train), np.array(Y_train)\n",
        "\n",
        "def Build_Model(X_train, Y_train, model_num):\n",
        "  model = Sequential()\n",
        "  if model_num == 1:\n",
        "    model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1],X_train.shape[2])))\n",
        "    model.add(LSTM(50))\n",
        "    model.add(Dense(25))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    model.fit(X_train, Y_train, batch_size=64, epochs=100)\n",
        "  elif model_num == 2:\n",
        "    model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1],X_train.shape[2])))\n",
        "    model.add(LSTM(50))\n",
        "    model.add(Dense(25))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    model.fit(X_train, Y_train, batch_size=64, epochs=100)\n",
        "  elif model_num == 3:\n",
        "    model.add(LSTM(30, return_sequences=True, input_shape=(X_train.shape[1],X_train.shape[2])))\n",
        "    model.add(LSTM(30))\n",
        "    model.add(Dense(15))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    model.fit(X_train, Y_train, batch_size=64, epochs=100)\n",
        "  elif model_num == 4:\n",
        "    model.add(LSTM(100, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(LSTM(100, return_sequences=False))\n",
        "    model.add(Dense(50))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1))\n",
        "    optimizer = Adam(lr=0.001)\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    model.fit(X_train, Y_train, batch_size=16, epochs=100, verbose=1)\n",
        "  elif model_num == 5:\n",
        "    model.add(LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dense(25))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    model.fit(X_train, Y_train, batch_size=32, epochs=100)\n",
        "  elif model_num == 6:\n",
        "    model.add(LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dense(25))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    model.fit(X_train, Y_train, batch_size=32, epochs=100)\n",
        "  elif model_num == 7:\n",
        "    model.add(LSTM(30, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(LSTM(30))\n",
        "    model.add(Dense(15))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    model.fit(X_train, Y_train, batch_size=32, epochs=100)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "XUOKPzNOwNeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main"
      ],
      "metadata": {
        "id": "PsAD_orrm_6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, X, scaler_open):\n",
        "\n",
        "  future_predictions = model.predict(X)\n",
        "  future_predictions = scaler_open.inverse_transform(future_predictions)\n",
        "  future_predictions = future_predictions.reshape(future_predictions.shape[1]*future_predictions.shape[0])\n",
        "\n",
        "  return future_predictions"
      ],
      "metadata": {
        "id": "RPWLsfiZoeyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for company, df_comp in tqdm(company_df_dict.items()):\n",
        "  print('\\n'+company+'\\n')\n",
        "\n",
        "  input_num = int(final_model.loc[final_model['company'] == company, 'input'])\n",
        "  model_num = int(final_model.loc[final_model['company'] == company, 'model'])\n",
        "\n",
        "  params_path = home + 'final_model_params/'\n",
        "  # normalization and save scaler\n",
        "  scaler_open, scaler = None, None\n",
        "  # for col in df_comp.columns:\n",
        "  #   scaler = MinMaxScaler(feature_range=(0,1))\n",
        "  #   df_comp[col] = scaler.fit_transform(df_comp.filter([col]).values)\n",
        "  #   joblib.dump(scaler, params_path + company + '/' + f'scaler_{col}.z')\n",
        "  scalers = {}\n",
        "  for col in df_comp.columns:\n",
        "      scaler = joblib.load(params_path + company + '/' + f'scaler_{col}.z')\n",
        "      scalers[col] = scaler\n",
        "\n",
        "  model = load_model(params_path + company + '/model.keras')\n",
        "  for col in df_comp.columns:\n",
        "      if col in scalers:\n",
        "          df_comp[col] = scalers[col].transform(df_comp.filter([col]).values)\n",
        "\n",
        "  predictions = predict(model, df_comp.values, scalers['Open'])\n",
        "\n",
        "  #  train and save final model\n",
        "  # split_point = math.ceil(len(df_comp)* .9)\n",
        "  # X_train, Y_train = Split_Data(df_comp, input_num, 5, split_point)\n",
        "  # model = Build_Model(X_train, Y_train, model_num)\n",
        "  # model.save(params_path + company + '/model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "Uv_GclWQg6No",
        "outputId": "fd756b53-7689-425f-d53e-640604021def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "大立光\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b21e325b17f8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mscalers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_comp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcompany\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf'scaler_{col}.z'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0mscalers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/find_optimal/final_model_params/大立光/scaler_Open.z'"
          ]
        }
      ]
    }
  ]
}